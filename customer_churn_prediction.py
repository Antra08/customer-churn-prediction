# -*- coding: utf-8 -*-
"""customer churn prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XU1LQNTeDwF1xR06g5Gbw8kWjp28kMHT
"""

from google.colab import drive
drive.mount('/content/drive')

my_folder = "/content/drive/MyDrive/churn prediction"

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/churn prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv")

df

df.head()

df.info()

df.value_counts()

pd.options.display.max_rows = None
df

pd.options.display.max_columns = None
df.head()

df.shape

df.isnull().sum()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df.isnull().sum()

df.dropna(subset=['TotalCharges'], inplace=True)

df.isnull().values.any()

df.isnull().sum()

df.drop('customerID', axis=1, inplace=True)

df = df['Churn'].map({'yes':1, 'No':0})

print(type(df))

cat_cols = df.select_dtypes(include='object').columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)

df.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

x = df_encoded.drop('Churn_Yes', axis=1)
y = df_encoded['Churn_Yes']

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_scaled, y_train)
lr_preds = lr.predict(X_test_scaled)
print("Logistic Regression:\n", classification_report(y_test, lr_preds))

xgb = XGBClassifier(eval_metric='logloss')
xgb.fit(X_train_scaled, y_train)
xgb_preds = xgb.predict(X_test_scaled)
print("XGBoost:\n", classification_report(y_test, xgb_preds))

lr_proba = lr.predict_proba(X_test_scaled)[:, 1]
xgb_proba = xgb.predict_proba(X_test_scaled)[:, 1]

print("Logistic Regression AUC:", roc_auc_score(y_test, lr_proba))
print("XGBoost AUC:", roc_auc_score(y_test, xgb_proba))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, precision_recall_curve, auc

fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_proba)
auc_lr = roc_auc_score(y_test, lr_proba)

plt.figure(figsize=(6, 5))
plt.plot(fpr_lr, tpr_lr, color='blue', label=f"AUC = {auc_lr:.2f}")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Logistic Regression - ROC Curve")
plt.legend()
plt.grid(True)
plt.show()

precision_lr, recall_lr, _ = precision_recall_curve(y_test, lr_proba)

plt.figure(figsize=(6, 5))
plt.plot(recall_lr, precision_lr, color='blue')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Logistic Regression - Precision-Recall Curve")
plt.grid(True)
plt.show()

fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_proba)
auc_xgb = roc_auc_score(y_test, xgb_proba)

plt.figure(figsize=(6, 5))
plt.plot(fpr_xgb, tpr_xgb, color='green', label=f"AUC = {auc_xgb:.2f}")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost - ROC Curve")
plt.legend()
plt.grid(True)
plt.show()

precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, xgb_proba)

plt.figure(figsize=(6, 5))
plt.plot(recall_xgb, precision_xgb, color='green')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("XGBoost - Precision-Recall Curve")
plt.grid(True)
plt.show()

!pip install shap

import shap

# Create LinearExplainer for Logistic Regression
explainer_lr = shap.LinearExplainer(lr, X_train_scaled, feature_perturbation="interventional")

# Get SHAP values for test data
shap_values_lr = explainer_lr.shap_values(X_test_scaled)

shap.summary_plot(shap_values_lr, X_test_scaled, feature_names=x.columns, plot_type="bar")
shap.summary_plot(shap_values_lr, X_test_scaled, feature_names=x.columns)

# Create TreeExplainer for XGBoost
explainer_xgb = shap.Explainer(xgb, X_train_scaled)

# Get SHAP values for test data
shap_values_xgb = explainer_xgb(X_test_scaled)

# Plot summary plot for XGBoost
shap.summary_plot(shap_values_xgb, X_test_scaled, feature_names=x.columns, plot_type="bar")
shap.summary_plot(shap_values_xgb, X_test_scaled, feature_names=x.columns)

!pip install lime

from lime import lime_tabular
import numpy as np

# Create a LIME explainer on your training data
explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train_scaled),
    feature_names=x.columns,
    class_names=['No Churn', 'Churn'],
    mode='classification'
)

i = 0  # index of test instance to explain
exp = explainer.explain_instance(
    data_row=X_test_scaled[i],
    predict_fn=xgb.predict_proba,  # use the model's predict_proba function
    num_features=5  # number of features to show in explanation
)

# Show explanation as a list

exp.show_in_notebook(show_table=True)

exp.save_to_file('lime_explanation.html')

exp = explainer.explain_instance(
    data_row=X_test_scaled[i],
    predict_fn=lr.predict_proba,  # Logistic Regression model
    num_features=5
)
exp.show_in_notebook(show_table=True)

exp = explainer.explain_instance(
    data_row=X_test_scaled[i],
    predict_fn=xgb.predict_proba,  # XGBoost model
    num_features=5
)
exp.show_in_notebook(show_table=True)